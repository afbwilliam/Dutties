<html>
<head>
<title>CONOPT4 Options</title>
</head>
<body>
<h2>CONOPT4 Options</h2>
For more information about this solver please inspect the
complete <a href="docs/solvers/conopt4.pdf">CONOPT4 manual</a>.

<h2>Summary of CONOPT4 Options</h2>
<table>
<tr><th height=75 valign=bottom colspan=2 align=left><h3>Algorithmic options</h3></th></tr>
<tr><td><a href="#CONOPT4Flg_Crash_Basis">
Flg_Crash_Basis</a></td>
<td> Flag for crashing an initial basis without fixed slacks</td></tr>
<tr><td><a href="#CONOPT4Flg_Crash_Slack">
Flg_Crash_Slack</a></td>
<td> Flag for selecting initial basis as Crash-triangular variables plus slacks.</td></tr>
<tr><td><a href="#CONOPT4Flg_Crash_Tria">
Flg_Crash_Tria</a></td>
<td> Flag for using the triangular crash method.</td></tr>
<tr><td><a href="#CONOPT4Flg_LinMod">
Flg_LinMod</a></td>
<td> Flag for allowing the Linear Feasibility Model.</td></tr>
<tr><td><a href="#CONOPT4Flg_NoPen">
Flg_NoPen</a></td>
<td> Flag for allowing the Model without penalty constraints</td></tr>
<tr><td><a href="#CONOPT4Flg_Perturb">
Flg_Perturb</a></td>
<td> Flag for use of perturbations to compute 2nd derivatives in SQP method.</td></tr>
<tr><td><a href="#CONOPT4Flg_PostTria">
Flg_PostTria</a></td>
<td> Pre-processor flag for identifying and using post-triangular equations.</td></tr>
<tr><td><a href="#CONOPT4Flg_Pretria">
Flg_Pretria</a></td>
<td> Pre-processor flag for identifying and using pre-triangular equations.</td></tr>
<tr><td><a href="#CONOPT4Flg_RedHess">
Flg_RedHess</a></td>
<td> Flag for approximating Reduced Hessian information for incoming superbasics.</td></tr>
<tr><td><a href="#CONOPT4Flg_SLPMode">
Flg_SLPMode</a></td>
<td> Flag for enabling SLP mode.</td></tr>
<tr><td><a href="#CONOPT4Flg_SQPMode">
Flg_SQPMode</a></td>
<td> Flag for enabling of SQP mode.</td></tr>
<tr><td><a href="#CONOPT4Flg_Square">
Flg_Square</a></td>
<td> Flag for Square System. Alternative to defining modeltype=CNS in GAMS</td></tr>
<tr><td><a href="#CONOPT4Flg_SteepEdge">
Flg_SteepEdge</a></td>
<td> Flag for turning Steepest Edge on.</td></tr>
<tr><td><a href="#CONOPT4Frq_Rescale">
Frq_Rescale</a></td>
<td> Rescaling frequency.</td></tr>
<tr><td><a href="#CONOPT4Lim_Err_2DDir">
Lim_Err_2DDir</a></td>
<td> Limit on errors in Directional Second Derivative evaluation.</td></tr>
<tr><td><a href="#CONOPT4Lim_Err_Fnc_Drv">
Lim_Err_Fnc_Drv</a></td>
<td> Limit on number of function evaluation errors. Overwrites GAMS Domlim option</td></tr>
<tr><td><a href="#CONOPT4Lim_Err_Hessian">
Lim_Err_Hessian</a></td>
<td> Limit on errors in Hessian evaluation.</td></tr>
<tr><td><a href="#CONOPT4Lim_Iteration">
Lim_Iteration</a></td>
<td> Maximum number of iterations. Overwrites GAMS Iterlim option.</td></tr>
<tr><td><a href="#CONOPT4Lim_NewSuper">
Lim_NewSuper</a></td>
<td> Maximum number of new superbasic variables added in one iteration.</td></tr>
<tr><td><a href="#CONOPT4Lim_RedHess">
Lim_RedHess</a></td>
<td> Maximum number of superbasic variables in the approximation to the Reduced Hessian.</td></tr>
<tr><td><a href="#CONOPT4Lim_SlowPrg">
Lim_SlowPrg</a></td>
<td> Limit on number of iterations with slow progress (relative less than Tol_Obj_Change).</td></tr>
<tr><td><a href="#CONOPT4Lim_StallIter">
Lim_StallIter</a></td>
<td> Limit on the number of stalled iterations.</td></tr>
<tr><td><a href="#CONOPT4Lim_Start_Degen">
Lim_Start_Degen</a></td>
<td> Limit on number of degenerate iterations before starting degeneracy breaking strategy.</td></tr>
<tr><td><a href="#CONOPT4Lim_Time">
Lim_Time</a></td>
<td> Time Limit. Overwrites the GAMS Reslim option.</td></tr>
<tr><td><a href="#CONOPT4Lim_Variable">
Lim_Variable</a></td>
<td> Upper bound on solution values and equation activity levels</td></tr>
<tr><td><a href="#CONOPT4Mtd_Dbg_1Drv">
Mtd_Dbg_1Drv</a></td>
<td> Method used by the function and derivative debugger.</td></tr>
<tr><td><a href="#CONOPT4Mtd_RedHess">
Mtd_RedHess</a></td>
<td> Method for initializing the diagonal of the approximate Reduced Hessian</td></tr>
<tr><td><a href="#CONOPT4Mtd_Scale">
Mtd_Scale</a></td>
<td> Method used for scaling.</td></tr>
<tr><td><a href="#CONOPT4Mtd_Step_Phase0">
Mtd_Step_Phase0</a></td>
<td> Method used to determine the step in Phase 0.</td></tr>
<tr><td><a href="#CONOPT4Mtd_Step_Tight">
Mtd_Step_Tight</a></td>
<td> Method used to determine the maximum step while tightening tolerances.</td></tr>
<tr><td><a href="#CONOPT4Tol_Bound">
Tol_Bound</a></td>
<td> Bound filter tolerance for solution values close to a bound.</td></tr>
<tr><td><a href="#CONOPT4Tol_Feas_Max">
Tol_Feas_Max</a></td>
<td> Maximum feasibility tolerance (after scaling).</td></tr>
<tr><td><a href="#CONOPT4Tol_Feas_Min">
Tol_Feas_Min</a></td>
<td> Minimum feasibility tolerance (after scaling).</td></tr>
<tr><td><a href="#CONOPT4Tol_Feas_Tria">
Tol_Feas_Tria</a></td>
<td> Feasibility tolerance for triangular equations.</td></tr>
<tr><td><a href="#CONOPT4Tol_Fixed">
Tol_Fixed</a></td>
<td> Bound tolerance for defining variables as fixed.</td></tr>
<tr><td><a href="#CONOPT4Tol_Jac_Min">
Tol_Jac_Min</a></td>
<td> Filter for small Jacobian elements to be ignored during scaling.</td></tr>
<tr><td><a href="#CONOPT4Tol_Linesearch">
Tol_Linesearch</a></td>
<td> Accuracy of One-dimensional search.</td></tr>
<tr><td><a href="#CONOPT4Tol_Obj_Acc">
Tol_Obj_Acc</a></td>
<td> Relative accuracy of the objective function.</td></tr>
<tr><td><a href="#CONOPT4Tol_Obj_Change">
Tol_Obj_Change</a></td>
<td> Limit for relative change in objective for well-behaved iterations.</td></tr>
<tr><td><a href="#CONOPT4Tol_Optimality">
Tol_Optimality</a></td>
<td> Optimality tolerance for reduced gradient when feasible.</td></tr>
<tr><td><a href="#CONOPT4Tol_Opt_Infeas">
Tol_Opt_Infeas</a></td>
<td> Optimality tolerance for reduced gradient when infeasible.</td></tr>
<tr><td><a href="#CONOPT4Tol_Piv_Abs">
Tol_Piv_Abs</a></td>
<td> Absolute pivot tolerance.</td></tr>
<tr><td><a href="#CONOPT4Tol_Piv_Abs_Ini">
Tol_Piv_Abs_Ini</a></td>
<td> Absolute Pivot Tolerance for building initial basis.</td></tr>
<tr><td><a href="#CONOPT4Tol_Piv_Abs_NLTr">
Tol_Piv_Abs_NLTr</a></td>
<td> Absolute pivot tolerance for nonlinear elements in pre-triangular equations.</td></tr>
<tr><td><a href="#CONOPT4Tol_Piv_Rel">
Tol_Piv_Rel</a></td>
<td> Relative pivot tolerance during basis factorizations.</td></tr>
<tr><td><a href="#CONOPT4Tol_Piv_Rel_Ini">
Tol_Piv_Rel_Ini</a></td>
<td> Relative Pivot Tolerance for building initial basis</td></tr>
<tr><td><a href="#CONOPT4Tol_Piv_Rel_Updt">
Tol_Piv_Rel_Updt</a></td>
<td> Relative pivot tolerance during basis updates.</td></tr>
<tr><td><a href="#CONOPT4Tol_Scale2D_Min">
Tol_Scale2D_Min</a></td>
<td> Lower bound for scale factors based on large 2nd derivatives.</td></tr>
<tr><td><a href="#CONOPT4Tol_Scale_Max">
Tol_Scale_Max</a></td>
<td> Upper bound on scale factors.</td></tr>
<tr><td><a href="#CONOPT4Tol_Scale_Min">
Tol_Scale_Min</a></td>
<td> Lower bound for scale factors computed from values and 1st derivatives.</td></tr>
<tr><td><a href="#CONOPT4Tol_Scale_Var">
Tol_Scale_Var</a></td>
<td> Lower bound on x in x*Jac used when scaling.</td></tr>
<tr><td><a href="#CONOPT4Tol_Zero">
Tol_Zero</a></td>
<td> Zero filter for Jacobian elements and inversion results.</td></tr>
<tr><th height=75 valign=bottom colspan=2 align=left><h3>Debugging options</h3></th></tr>
<tr><td><a href="#CONOPT4Lim_Dbg_1Drv">
Lim_Dbg_1Drv</a></td>
<td> Flag for debugging of first derivatives</td></tr>
<tr><td><a href="#CONOPT4Lim_Hess_Est">
Lim_Hess_Est</a></td>
<td> Upper bound on second order terms.</td></tr>
<tr><td><a href="#CONOPT4Lim_Msg_Dbg_1Drv">
Lim_Msg_Dbg_1Drv</a></td>
<td> Limit on number of error messages from function and derivative debugger.</td></tr>
<tr><th height=75 valign=bottom colspan=2 align=left><h3>Output options</h3></th></tr>
<tr><td><a href="#CONOPT4Frq_Log_Simple">
Frq_Log_Simple</a></td>
<td> Frequency for log-lines for non-SLP/SQP iterations.</td></tr>
<tr><td><a href="#CONOPT4Frq_Log_SlpSqp">
Frq_Log_SlpSqp</a></td>
<td> Frequency for log-lines for SLP or SQP iterations.</td></tr>
<tr><td><a href="#CONOPT4Lim_Msg_LargeDrv">
Lim_Msg_LargeDrv</a></td>
<td> Limit on number of error messages related to large Jacobian elements.</td></tr>
<tr><th height=75 valign=bottom colspan=2 align=left><h3>Interface options</h3></th></tr>
<tr><td><a href="#CONOPT4cooptfile">
cooptfile</a></td>
<td></td></tr>
<tr><td><a href="#CONOPT4Flg_2DDir">
Flg_2DDir</a></td>
<td> Flag for computing and using directional 2nd derivatives.</td></tr>
<tr><td><a href="#CONOPT4Flg_Hessian">
Flg_Hessian</a></td>
<td> Flag for computing and using 2nd derivatives as Hessian of Lagrangian.</td></tr>
<tr><td><a href="#CONOPT4HEAPLIMIT">
HEAPLIMIT</a></td>
<td>Maximum Heap size in MB allowed</td></tr>
<tr><td><a href="#CONOPT4HessianMemFac">
HessianMemFac</a></td>
<td> Memory factor for Hessian generation: Skip if #Hessian elements > #Nonlinear Jacobian elements*HessianMemFac, 0 means unlimited.</td></tr>
<tr><td><a href="#CONOPT4THREAD2D">
THREAD2D</a></td>
<td>Number of threads used for second derivatives</td></tr>
<tr><td><a href="#CONOPT4THREADF">
THREADF</a></td>
<td>Number of threads used for function evaluation</td></tr>
<tr><td><a href="#CONOPT4threads">
threads</a></td>
<td>Number of threads used by Conopt internally</td></tr></table>
<h2>Detailed Descriptions of CONOPT4 Options</h2>

<h4><a name="CONOPT4cooptfile">
cooptfile</a>
<i> (string)</i> </h4><p>



<h4><a name="CONOPT4Flg_2DDir">
Flg_2DDir</a>
<i> (integer)</i>  Flag for computing and using directional 2nd derivatives.</h4><p>

If turned on, make directional second derivatives (Hessian matrix times
directional vector) available to CONOPT. The default is
on, but it will be turned off of the model has external
equations (defined with =X=) and the user has not provided
directional second derivatives. If both the Hessian of
the Lagrangian (see <a href="#Flg_Hessian">Flg_Hessian</a>) and
directional second derivatives are available then
CONOPT will use both: directional second derivatives are
used when the expected number of iterations in the SQP
sub-solver is low and the Hessian is used when the
expected number of iterations is large.

<br><i>(default = auto)</i>

<h4><a name="CONOPT4Flg_Crash_Basis">
Flg_Crash_Basis</a>
<i> (integer)</i>  Flag for crashing an initial basis without fixed slacks</h4><p>

When turned on (1) CONOPT will try to crash a basis without fixed
slacks in the basis. Fixed slacks are only included in
a last round to fill linearly dependent rows. When turned
off (0), large infeasible slacks will be included in the initial
basis with preference for variables and slacks far from bound.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_Crash_Slack">
Flg_Crash_Slack</a>
<i> (integer)</i>  Flag for selecting initial basis as Crash-triangular variables plus slacks.</h4><p>

When turned on together with <a href="#Flg_Crash_tria">Flg_Crash_tria</a>
CONOPT will use the triangular crash
procedure and select the initial basis as the
crash-triangular variables plus slacks in all remaining rows.

<br><i>(default = 0)</i>

<h4><a name="CONOPT4Flg_Crash_Tria">
Flg_Crash_Tria</a>
<i> (integer)</i>  Flag for using the triangular crash method.</h4><p>

When turned on CONOPT will try to crash a triangular basis
using ideas by Gould and Reid. The procedure relies
on identifying and using good initial values provided by
the modeler and only assigning values to variables that
are not initialized. Should only be used when several
important variables have been given reasonable initial
values. The sum of infeasibilities may for some models
grow during the crash procedure, so modelers are advised
that the option should be used with caution.

<br><i>(default = 0)</i>

<h4><a name="CONOPT4Flg_Hessian">
Flg_Hessian</a>
<i> (integer)</i>  Flag for computing and using 2nd derivatives as Hessian of Lagrangian.</h4><p>

If turned on, compute the structure of the Hessian of the
Lagrangian and make it available to CONOPT. The default is
usually on, but it will be turned off if the model has
external equations (defined with =X=) or cone constraints
(defined with =C=) or if the Hessian becomes too dense.
See also <a href="#Flg_2DDir">Flg_2DDir</a> and <a href="#HessianMemFac">HessianMemFac</a>.

<br><i>(default = auto)</i>

<h4><a name="CONOPT4Flg_LinMod">
Flg_LinMod</a>
<i> (integer)</i>  Flag for allowing the Linear Feasibility Model.</h4><p>

When turned on (the default) CONOPT will create and solve a linear
feasibility model if the linear constraints outside the
pre- and post-triangle are infeasible in the initial point.
This is a safe way to start the solution process.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_NoPen">
Flg_NoPen</a>
<i> (integer)</i>  Flag for allowing the Model without penalty constraints</h4><p>

When turned on (the default) CONOPT will create and solve a
smaller model without the penalty constraints and variables
if the remaining constraints are infeasible in the initial point.
This is often a faster way to start the solution process.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_Perturb">
Flg_Perturb</a>
<i> (integer)</i>  Flag for use of perturbations to compute 2nd derivatives in SQP method.</h4><p>

If turned on (1) CONOPT may use perturbations of the
Jacobian to compute directional 2nd derivatives. Is
used to turn the SQP procedure on if there are no
other methods available for computing 2nd derivative
information. With GAMS it is only relevant for models
with external equations defined with =X=.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_PostTria">
Flg_PostTria</a>
<i> (integer)</i>  Pre-processor flag for identifying and using post-triangular equations.</h4><p>

When turned on (the default) CONOPT will try to identify
post-triangular equations. Otherwise this phase is ignored.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_Pretria">
Flg_Pretria</a>
<i> (integer)</i>  Pre-processor flag for identifying and using pre-triangular equations.</h4><p>

When turned on (the default) CONOPT will try to identify
pre-triangular equations. Otherwise this phase is ignored.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_RedHess">
Flg_RedHess</a>
<i> (integer)</i>  Flag for approximating Reduced Hessian information for incoming superbasics.</h4><p>

If Flg_RedHess is turned on (1) CONOPT will try to estimate second order
information in the reduced Hessian for incoming superbasic variables based
on directional second derivatives. This is more costly than
the standard method described under <a href="#Mtd_RedHess">Mtd_RedHess</a>.

<br><i>(default = 0)</i>

<h4><a name="CONOPT4Flg_SLPMode">
Flg_SLPMode</a>
<i> (integer)</i>  Flag for enabling SLP mode.</h4><p>

If Flg_SLPMode is on (the default) then the SLP (sequential
linear programming) sub-solver can be used, otherwise
it is turned off.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_SQPMode">
Flg_SQPMode</a>
<i> (integer)</i>  Flag for enabling of SQP mode.</h4><p>

If Flg_SQPMode is on (the default) then the SQP (sequential
quadratic programming) sub-solver can be used, otherwise
it is turned off.

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Flg_Square">
Flg_Square</a>
<i> (integer)</i>  Flag for Square System. Alternative to defining modeltype=CNS in GAMS</h4><p>

When turned on the modeler declares that this is a
square system, i.e. the number of non-fixed variables
must be equal to the number of constraints, no bounds
must be active in the final solution, and the basis
selected from the non-fixed variables must always be
nonsingular.

<br><i>(default = 0)</i>

<h4><a name="CONOPT4Flg_SteepEdge">
Flg_SteepEdge</a>
<i> (integer)</i>  Flag for turning Steepest Edge on.</h4><p>

Flag used to turn steepest edge pricing on (1)
or off (0). Steepest edge pricing makes the individual
iterations more expensive but it may decrease their number.
Only experimentation can show if it is worth while.

<br><i>(default = 0)</i>

<h4><a name="CONOPT4Frq_Log_Simple">
Frq_Log_Simple</a>
<i> (integer)</i>  Frequency for log-lines for non-SLP/SQP iterations.</h4><p>

Frq_Log_Simple and Frq_Log_SlpSqp can be used to control the amount of
iteration send to the log file.
The non-SLP/SQP iterations, i.e. iterations in phase 0, 1, and 3, are
usually fast and writing a log line for each iteration may be too much,
especially for smaller models. The default value for the log
frequency for these iterations is therefore set to 10 for small models,
5 for models with more than 500 constraints or 1000 variables and 1
for models with more than 2000 constraints or 3000 variables.

<br><i>(default = auto)</i>

<h4><a name="CONOPT4Frq_Log_SlpSqp">
Frq_Log_SlpSqp</a>
<i> (integer)</i>  Frequency for log-lines for SLP or SQP iterations.</h4><p>

Frq_Log_Simple and Frq_Log_SlpSqp can be used to control the amount of
iteration send to the log file.
Iterations using the SLP and/or SQP sub-solver, i.e. iterations in
phase 2 and 4, may involve several inner iterations and the work
per iteration is therefore larger than for the non-SLP/SQP iterations
and it may be relevant to write log lines more frequently. The default
value for the log frequency is therefore 5 for small models and 1 for
models with more than 500 constraints or 1000 variables.

<br><i>(default = auto)</i>

<h4><a name="CONOPT4Frq_Rescale">
Frq_Rescale</a>
<i> (integer)</i>  Rescaling frequency.</h4><p>

The row and column scales are recalculated at least
every Frq_Rescale new point (degenerate iterations do not
count), or more frequently if conditions require it.

<br><i>(default = 5)</i>

<h4><a name="CONOPT4HEAPLIMIT">
HEAPLIMIT</a>
<i> (real)</i> Maximum Heap size in MB allowed</h4><p>


<br><i>Range: [0,maxdouble]</i>

<br><i>(default = 1e20)</i>

<h4><a name="CONOPT4HessianMemFac">
HessianMemFac</a>
<i> (real)</i>  Memory factor for Hessian generation: Skip if #Hessian elements > #Nonlinear Jacobian elements*HessianMemFac, 0 means unlimited.</h4><p>

The Hessian of the Lagrangian is considered too dense
and is not passed on to CONOPT if the number of nonzero
elements in the Hessian of the Lagrangian is greater
than the number of nonlinear Jacobian elements multiplied
by HessianMemFac. The assumption is that a very dense Hessian is
expensive to both compute and use.
If HessianMemFac = 0.0 then there is no limit on the number of
Hessian elements.

<br><i>(default = 10)</i>

<h4><a name="CONOPT4Lim_Dbg_1Drv">
Lim_Dbg_1Drv</a>
<i> (integer)</i>  Flag for debugging of first derivatives</h4><p>

 Lim_Dbg_1Drv controls how often the derivatives are tested.
 Debugging of derivatives is only relevant for user-written
 functions in external equations defined with =X=.
 The amount of debugging is controlled by
 <a href="#Mtd_Dbg_1Drv">Mtd_Dbg_1Drv</a>.
 See <a href="#Lim_Hess_Est">Lim_Hess_Est</a> for a definition of when derivatives
 are considered wrong.

<br><i>(default = 0)</i>
<table>
<tr valign="top"><td width=20 align=right>-1</td><td>.
 The derivatives are tested in the initial point
 only.</td></tr>
<tr valign="top"><td width=20 align=right>0</td><td>.
 No debugging</td></tr>
<tr valign="top"><td width=20 align=right>+n</td><td>.
 The derivatives are tested in all iterations
 that can be divided by Lim_Dbg_1Drv, provided the
 derivatives are computed in this iteration.
 (During phase 0, 1, and 3 derivatives are only
 computed when it appears to be necessary.)</td></tr>
</table>

<h4><a name="CONOPT4Lim_Err_2DDir">
Lim_Err_2DDir</a>
<i> (integer)</i>  Limit on errors in Directional Second Derivative evaluation.</h4><p>

If the evaluation of Directional Second Derivatives (Hessian
information in a particular direction) has failed more than Lim_Err_2DDir
times CONOPT will not attempt to evaluate them any more and will switch to
methods that do not use Directional Second Derivatives.
Note that second order information may not be defined even if function and derivative values
are well-defined, e.g. in an expression like power(x,1.5) at x=0.

<br><i>(default = 10)</i>

<h4><a name="CONOPT4Lim_Err_Fnc_Drv">
Lim_Err_Fnc_Drv</a>
<i> (integer)</i>  Limit on number of function evaluation errors. Overwrites GAMS Domlim option</h4><p>

Function values and their derivatives are assumed to be defined
in all points that satisfy the bounds of the model. If the function
value or a derivative is not defined in a point CONOPT will
try to recover by going back to a previous safe point (if one exists), but it will
not do it more than at most Lim_Err_Fnc_Drv times. If CONOPT is stopped
by functions or derivatives not being defined it will
return with a intermediate infeasible or intermediate non-optimal model status.

<br><i>(default = GAMS DomLim)</i>

<h4><a name="CONOPT4Lim_Err_Hessian">
Lim_Err_Hessian</a>
<i> (integer)</i>  Limit on errors in Hessian evaluation.</h4><p>

If the evaluation of Hessian information has failed more than Lim_Err_Hessian times
CONOPT will not attempt to evaluate it any more and will switch to
methods that do not use the Hessian.
Note that second order information may not be defined even if function and derivative values
are well-defined, e.g. in an expression like power(x,1.5) at x=0.

<br><i>(default = 10)</i>

<h4><a name="CONOPT4Lim_Hess_Est">
Lim_Hess_Est</a>
<i> (real)</i>  Upper bound on second order terms.</h4><p>

The function and derivative debugger (see <a href="#Lim_Dbg_1Drv">Lim_Dbg_1Drv</a>)
tests if derivatives computed using the modelers routine are
sufficiently close to the values computed using finite
differences. The term for the acceptable difference includes
a second order term and uses Lim_Hess_Est as an estimate of the upper bound on
second order derivatives in the model. Larger Lim_Hess_Est
values will allow larger deviations between the user-defined derivatives
and the numerically computed derivatives.

<br><i>(default = 1.e4)</i>

<h4><a name="CONOPT4Lim_Iteration">
Lim_Iteration</a>
<i> (integer)</i>  Maximum number of iterations. Overwrites GAMS Iterlim option.</h4><p>

The iteration limit can be used to prevent models from spending too many
resources. You should note that the cost of the different types of CONOPT
iterations (phase 0 to 4) can be very different so the time limit
(GAMS Reslim or option <a href="#Lim_Time">Lim_Time</a>) is
often a better stopping criterion. However, the iteration limit is better
for reproducing solution behavior across machines.

<br><i>(default = GAMS IterLim)</i>

<h4><a name="CONOPT4Lim_Msg_Dbg_1Drv">
Lim_Msg_Dbg_1Drv</a>
<i> (integer)</i>  Limit on number of error messages from function and derivative debugger.</h4><p>

The function and derivative debugger (see <a href="#Lim_Dbg_1Drv">Lim_Dbg_1Drv</a>)
may find a very large number of errors, all derived from the same source.
To avoid very large amounts of output CONOPT will stop the debugger
after Lim_Msg_Dbg_1Drv error(s) have been found.

<br><i>(default = 10)</i>

<h4><a name="CONOPT4Lim_Msg_LargeDrv">
Lim_Msg_LargeDrv</a>
<i> (integer)</i>  Limit on number of error messages related to large Jacobian elements.</h4><p>

Very large derivatives (Jacobian elements) in a model will lead to
numerical difficulties and most likely to inaccurate primal and/or dual solutions.
CONOPT is therefore imposing an upper bound on the value of all
derivatives. This bound is 1.e30.
If the bound is violated CONOPT will return with an
intermediate infeasible or intermediate non-optimal
solution and it will issue error messages for all the violating
Jacobian elements, up to a limit of Lim_Msg_LargeDrv error messages.

<br><i>(default = 10)</i>

<h4><a name="CONOPT4Lim_NewSuper">
Lim_NewSuper</a>
<i> (integer)</i>  Maximum number of new superbasic variables added in one iteration.</h4><p>

When there has been a sufficient reduction
in the reduced gradient in one subspace new non-basics
can be selected to enter the superbasis.
The ones with largest reduced gradient of proper sign
are selected, up to a limit.
If Lim_NewSuper is positive then the limit is min(500,Lim_NewSuper).
If Lim_NewSuper is zero (the default) then the limit is selected
dynamically by CONOPT depending on model characteristics.

<br><i>(default = auto)</i>

<h4><a name="CONOPT4Lim_RedHess">
Lim_RedHess</a>
<i> (integer)</i>  Maximum number of superbasic variables in the approximation to the Reduced Hessian.</h4><p>

CONOPT uses and stores a dense lower-triangular matrix
as an approximation to the Reduced Hessian. The rows and columns
correspond to the superbasic variable. This matrix
can use a large amount of memory and computations involving
the matrix can be time consuming so CONOPT imposes a
limit on on the size. The limit is Lim_RedHess if Lim_RedHess is defined by the
modeler and otherwise a value determined from the overall
size of the model.
If the number of superbasics exceeds the limit, CONOPT
will switch to a method based on a combination of SQP and
Conjugate Gradient iterations assuming some kind of second order
information is available. If no second order information
is available CONOPT will use a quasi-Newton method on
a subset of the superbasic variables and rotate the subset
as the reduced gradient becomes small.

<br><i>(default = auto)</i>

<h4><a name="CONOPT4Lim_SlowPrg">
Lim_SlowPrg</a>
<i> (integer)</i>  Limit on number of iterations with slow progress (relative less than Tol_Obj_Change).</h4><p>

The optimization is stopped if the relative change in objective
is less than <a href="#Tol_Obj_Change">Tol_Obj_Change</a> for Lim_SlowPrg
consecutive well-behaved iterations.

<br><i>(default = 20)</i>

<h4><a name="CONOPT4Lim_StallIter">
Lim_StallIter</a>
<i> (integer)</i>  Limit on the number of stalled iterations.</h4><p>

An iteration  is considered a stalled iteration it there
is no change in objective because the linesearch is
limited by nonlinearities or numerical difficulties.
Stalled iterations will have Step = 0 and F in the OK
column of the log file.
After a stalled iteration CONOPT will usually
try various heuristics to get a better basis and a
better search direction. However, the heuristics
may not work as intended or they may even return to the
original bad basis, so to prevent cycling CONOPT
stops after Lim_StallIter stalled iterations and returns an
Intermediate Infeasible or Intermediate Nonoptimal solution.

<br><i>(default = 100)</i>

<h4><a name="CONOPT4Lim_Start_Degen">
Lim_Start_Degen</a>
<i> (integer)</i>  Limit on number of degenerate iterations before starting degeneracy breaking strategy.</h4><p>

The default CONOPT pivoting strategy has focus on numerical
stability, but it can potentially cycle. When the number of
consecutive degenerate iterations exceeds Lim_Start_Degen CONOPT will
switch to a pivoting strategy that is guaranteed to break
degeneracy but with slightly weaker numerical properties.

<br><i>(default = 10)</i>

<h4><a name="CONOPT4Lim_Time">
Lim_Time</a>
<i> (real)</i>  Time Limit. Overwrites the GAMS Reslim option.</h4><p>

The upper bound on the total number of seconds that can
be used in the execution phase. There are only tests
for time limit once per iteration. The default value
is 10000. Lim_Time is overwritten by Reslim when called
from GAMS.

<br><i>Range: [0,maxdouble]</i>

<br><i>(default = GAMS ResLim)</i>

<h4><a name="CONOPT4Lim_Variable">
Lim_Variable</a>
<i> (real)</i>  Upper bound on solution values and equation activity levels</h4><p>

If the value of a variable, including the objective
function value and the value of slack variables, exceeds
Lim_Variable then the model is
considered to be unbounded and the optimization process
returns the solution with the large variable flagged
as unbounded.

<br><i>Range: [1.e5,1.e30]</i>

<br><i>(default = 1.e15)</i>

<h4><a name="CONOPT4Mtd_Dbg_1Drv">
Mtd_Dbg_1Drv</a>
<i> (integer)</i>  Method used by the function and derivative debugger.</h4><p>

The function and derivative debugger (turned on with
<a href="#Lim_Dbg_1Drv">Lim_Dbg_1Drv</a>) can perform a
fairly cheap test or a more extensive test, controlled
by Mtd_Dbg_1Drv. See <a href="#Lim_Hess_Est">Lim_Hess_Est</a> for a
definition of when derivatives are considered wrong.
All tests are performed in the current point found by
the optimization algorithm.

<br><i>(default = 0)</i>
<table>
<tr valign="top"><td width=20 align=right>0</td><td>.
Perform tests for sparsity pattern and tests that
the numerical values of the derivatives appear to be
correct. This is the default.</td></tr>
<tr valign="top"><td width=20 align=right>1</td><td>.
As 0 plus make extensive test to determine if
the functions and their derivatives are continuous
around the current point.
These tests are much more expensive and should only be
used of the cheap test does not find an error but
one is expected to exist.</td></tr>
</table>

<h4><a name="CONOPT4Mtd_RedHess">
Mtd_RedHess</a>
<i> (integer)</i>  Method for initializing the diagonal of the approximate Reduced Hessian</h4><p>

Each time a nonbasic variable is made superbasic a new
row and column is added to the approximate Reduced Hessian. The
off-diagonal elements are set to zero and the diagonal
to a value controlled by Mtd_RedHess:

<br><i>(default = 0)</i>
<table>
<tr valign="top"><td width=20 align=right>0</td><td>.
The new diagonal element is set to the geometric mean of
the existing diagonal elements.
This gives the new diagonal element an intermediate
value and new superbasic variables are therefore
not given any special treatment. The initial
steps should be of good size, but build-up of second order
information in the new sub-space may be slower. The larger diagonal
element may also in bad cases cause premature convergence.</td></tr>
<tr valign="top"><td width=20 align=right>1</td><td>.
The new diagonal elements is set to the minimum of the existing
diagonal elements.
This makes the new diagonal element small
and the importance of the new superbasic variable will therefore be high.
The initial steps can be rather small, but better
second order information in the new sub-space should be build up faster.</td></tr>
</table>

<h4><a name="CONOPT4Mtd_Scale">
Mtd_Scale</a>
<i> (integer)</i>  Method used for scaling.</h4><p>

CONOPT will by default use scaling of the equations
and variables of the model to improve the numerical
behavior of the solution algorithm and the accuracy
of the final solution (see also
<a href="#Frq_Rescale">Frq_Rescale</a>.)
The objective of the scaling process is to reduce
the values of all large primal and dual variables as well
as the values of all large first derivatives so they
become closer to 1. Small values are usually not
scaled up, see <a href="#Tol_Scale_Max">Tol_Scale_Max</a> and
<a href="#Tol_Scale_Min">Tol_Scale_Min</a>.
Scaling method 3 is recommended. The others are only
kept for backward compatibility.

<br><i>(default = 3)</i>
<table>
<tr valign="top"><td width=20 align=right>0</td><td>.
Scaling is based on repeatedly dividing the rows and
columns by the geometric means of the largest and
smallest elements in each row and column. Very
small elements less than <a href="#Tol_Jac_Min">Tol_Jac_Min</a>
are considered equal to Tol_Jac_Min.</td></tr>
<tr valign="top"><td width=20 align=right>1</td><td>.
Similar to 3 below, but the projection on the interval
[Tol_Scale_Min,Tol_Scale_Max] is applied at a different stage.
With method 1, abs(X)*abs(Jac) with small
X and very large Jac is scaled very aggressively
with a factor abs(Jac). With method 3, the scale
factor is abs(X)*abs(Jac). The difference is seen
in models with terms like Sqrt(X) close to X = 0.</td></tr>
<tr valign="top"><td width=20 align=right>2</td><td>.
As 1 but the terms are computed based on a moving
average of the squares X and Jac. The purpose of the
moving average is to keep the scale factor more
stable. This is often an advantage, but for models
with very large terms (large variables and in particular
large derivatives) in the initial point the averaging
process may not have enough time to bring
the scale factors into the right region.</td></tr>
<tr valign="top"><td width=20 align=right>3</td><td>.
Rows are first scaled by dividing by the largest term
in the row, then columns are scaled by dividing by
by the maximum of the largest term and the value of
the variable. A term is here defined as abs(X)*abs(Jac)
where X is the value of the variable and Jac is the
value of the derivative (Jacobian element). The scale
factor are then projected on the interval between
Tol_Scale_Min and Tol_Scale_Max.</td></tr>
</table>

<h4><a name="CONOPT4Mtd_Step_Phase0">
Mtd_Step_Phase0</a>
<i> (integer)</i>  Method used to determine the step in Phase 0.</h4><p>

The steplength used by the
Newton process in phase 0 is computed by one of two
alternative methods controlled by Mtd_Step_Phase0:

<br><i>(default = Auto)</i>
<table>
<tr valign="top"><td width=20 align=right>0</td><td>.
The standard ratio test method known from the Simplex method.
CONOPT adds small perturbations to the bounds to avoid very small
pivots and improve numerical stability. Linear constraints
that initially are feasible will remain feasible with this method.
It is the default method for optimization models.</td></tr>
<tr valign="top"><td width=20 align=right>1</td><td>.
A method based on bending (projecting the target values
of the basic variables on the bounds) until the sum of
infeasibilities is close to its minimum.
Linear constraints that initially are feasible
may become infeasible due to bending.
It is the default method for CNS models.</td></tr>
</table>

<h4><a name="CONOPT4Mtd_Step_Tight">
Mtd_Step_Tight</a>
<i> (integer)</i>  Method used to determine the maximum step while tightening tolerances.</h4><p>

The steplength used by the Newton process when
tightening tolerances is computed by one of two
alternative methods controlled by Mtd_Step_Tight:

<br><i>(default = 0)</i>
<table>
<tr valign="top"><td width=20 align=right>0</td><td>.
The standard ratio test method known from the Simplex method.
CONOPT adds small perturbations to the bounds to avoid very small
pivots and improve numerical stability. Linear constraints
that initially are feasible will remain feasible with this default method.</td></tr>
<tr valign="top"><td width=20 align=right>1</td><td>.
A method based on bending (projecting the target value
of the basic variables on the bounds) until the sum of
infeasibilities is close to its minimum.</td></tr>
</table>

<h4><a name="CONOPT4THREAD2D">
THREAD2D</a>
<i> (integer)</i> Number of threads used for second derivatives</h4><p>


<br><i>Range: [0,maxint]</i>

<br><i>(default = 1)</i>

<h4><a name="CONOPT4THREADF">
THREADF</a>
<i> (integer)</i> Number of threads used for function evaluation</h4><p>


<br><i>Range: [0,maxint]</i>

<br><i>(default = 1)</i>

<h4><a name="CONOPT4threads">
threads</a>
<i> (integer)</i> Number of threads used by Conopt internally</h4><p>


<br><i>Range: [0,maxint]</i>

<br><i>(default = GAMS Threads)</i>

<h4><a name="CONOPT4Tol_Bound">
Tol_Bound</a>
<i> (real)</i>  Bound filter tolerance for solution values close to a bound.</h4><p>

A variable is considered to be at a
bound if its distance from the bound is less than Tol_Bound
* Max(1,ABS(Bound)).

<br><i>Range: [3.e-13,1.e-5]</i>

<br><i>(default = 1.e-7)</i>

<h4><a name="CONOPT4Tol_Feas_Max">
Tol_Feas_Max</a>
<i> (real)</i>  Maximum feasibility tolerance (after scaling).</h4><p>

The feasibility tolerance used by CONOPT is dynamic.
As long as we are far from the optimal solution and make
large steps it is not necessary to compute intermediate
solutions very accurately. When we approach the optimum
and make smaller steps we need more accuracy. Tol_Feas_Max
is the upper bound on the dynamic feasibility tolerance
and <a href="#Tol_Feas_Min">Tol_Feas_Min</a> is the lower bound.
<p>
Tol_Feas_Max_Cur  RL  PR  The maximum tolerance on the residual of an equation
in the Newton iterations.
Tol_Feas_Max_P2  RL  PR  Temporary replacement for Tol_Feas_Max when we have problems
maintaining feasibility in phase 2. The maximum is
not relaxed above Tol_Feas_Max_P2 before the objective has
been brought below Safe_Obj.
Tol_Feas_Max_P1  RL  PR  Temporary replacement for Tol_Feas_Max when we have problems
maintaining feasibility of the small slacks in phase
1. The maximum is not relaxed above Tol_Feas_Max_P1 before
the sum of infeasibility objective has been brought
below Safe_Sinf in which case the maximum becomes Tol_Feas_Max_P2.

<br><i>Range: [1.e-10,1.e-3]</i>

<br><i>(default = 1.e-7)</i>

<h4><a name="CONOPT4Tol_Feas_Min">
Tol_Feas_Min</a>
<i> (real)</i>  Minimum feasibility tolerance (after scaling).</h4><p>

See <a href="#Tol_Feas_Max">Tol_Feas_Max</a> for a discussion of
the dynamic feasibility tolerances used by CONOPT.

<br><i>Range: [4.e-11,1.e-5]</i>

<br><i>(default = 4.e-10)</i>

<h4><a name="CONOPT4Tol_Feas_Tria">
Tol_Feas_Tria</a>
<i> (real)</i>  Feasibility tolerance for triangular equations.</h4><p>

Triangular equations are usually solved to an
accuracy of <a href="#Tol_Feas_Min">Tol_Feas_Min</a>.
However, if a variable reaches a
bound or a constraint only has pre-determined variables
then the feasibility tolerance can be relaxed to Tol_Feas_Tria.
Tol_Feas   RL  PR  The current tolerance on the residual of an equation in
the Newton iterations. Is initialized to Tol_Feas_Max and is
decreased and increased dynamically during the
optimization depending on the rate of improvements.
<p>
There are several tolerances, some scaled and some
unscaled, and the relationship is as follows:
Tol_Feas_Max and Tol_Feas_Min are the user-specified maximum and
minimum tolerances, respectively. They are not changed.
However, when there are tolerance problems, the maximum
is reduced temporarily. During phase 2 the maximum is
stored in Tol_Feas_Max_P2 and during phase 1 the maximum is in
Tol_Feas_Max_P1. The relationship is Tol_Feas_Min <= (Tol_Feas_Max_P2,Tol_Feas_Max_P1)
<= Tol_Feas_Max. The actual maximum, depending on the phase,
is stored in Tol_Feas_Max_Cur (Tol_Feas_Max_Cur is always equal to either
Tol_Feas_Max_P2 or Tol_Feas_Max_P1).
<p>
The dynamic feasibility tolerance, Tol_Feas, is always
between the tolerance bounds, i.e Tol_Feas_Min <= Tol_Feas <=
Tol_Feas_Max_Cur.
<p>
Tol_Feas_Slack  RL  IV  The critical value for 'Large Slacks'. Is set to
max( Tol_Bound, sqrt( Tol_Feas, Tol_Feas_Min ) ) when leaving
Coffs, and is not changed even if other tolerances
change.
<p>
Initial objective values and bounds

<br><i>Range: [3.e-13,1.e-4]</i>

<br><i>(default = 2.0e-8)</i>

<h4><a name="CONOPT4Tol_Fixed">
Tol_Fixed</a>
<i> (real)</i>  Bound tolerance for defining variables as fixed.</h4><p>

A variable is considered fixed if the
distance between the bounds is less than Tol_Fixed *
Max(1,Abs(Bound)). The tolerance is also used on
implied bounds (from converted inequalities) and these
implied bounds may be infeasible up to Tol_Fixed.
<p>
Accuracies for linesearch and updates

<br><i>Range: [3.e-13,1.e-5]</i>

<br><i>(default = 1.e-7)</i>

<h4><a name="CONOPT4Tol_Jac_Min">
Tol_Jac_Min</a>
<i> (real)</i>  Filter for small Jacobian elements to be ignored during scaling.</h4><p>

A Jacobian element is considered insignificant
if it is less than Tol_Jac_Min. The value is used to select which
small values are scaled up during scaling of the Jacobian.
Is only used with scaling method Mtd_Scale = 0.

<br><i>Range: [1.e-7,1.e-3]</i>

<br><i>(default = 1.e-5)</i>

<h4><a name="CONOPT4Tol_Linesearch">
Tol_Linesearch</a>
<i> (real)</i>  Accuracy of One-dimensional search.</h4><p>

The onedimensional search is stopped if the expected decrease in
then objective estimated from a quadratic approximation
is less than Tol_Linesearch times the decrease so far in this
onedimensional search.

<br><i>Range: [0.05,0.8]</i>

<br><i>(default = 0.2)</i>

<h4><a name="CONOPT4Tol_Obj_Acc">
Tol_Obj_Acc</a>
<i> (real)</i>  Relative accuracy of the objective function.</h4><p>

It is assumed that the objective function can be computed
to an accuracy of Tol_Obj_Acc * max( 1, abs(Objective) ).
Smaller changes in objective are considered to be
caused by round-off errors.

<br><i>Range: [3.0e-14,10.e-6]</i>

<br><i>(default = 3.0e-13)</i>

<h4><a name="CONOPT4Tol_Obj_Change">
Tol_Obj_Change</a>
<i> (real)</i>  Limit for relative change in objective for well-behaved iterations.</h4><p>

The change in objective in a well-behaved iteration
is considered small and the iteration counts as slow progress
if the change is less than Tol_Obj_Change * Max(1,Abs(Objective)).
See also <a href="#Lim_SlowPrg">Lim_SlowPrg</a>.

<br><i>Range: [3.0e-13,1.0e-5]</i>

<br><i>(default = 3.0e-12)</i>

<h4><a name="CONOPT4Tol_Optimality">
Tol_Optimality</a>
<i> (real)</i>  Optimality tolerance for reduced gradient when feasible.</h4><p>

The reduced gradient is considered zero and the
solution optimal if the largest superbasic component
of the reduced gradient is less than Tol_Optimality.

<br><i>Range: [3.e-13,1]</i>

<br><i>(default = 1.e-7)</i>

<h4><a name="CONOPT4Tol_Opt_Infeas">
Tol_Opt_Infeas</a>
<i> (real)</i>  Optimality tolerance for reduced gradient when infeasible.</h4><p>

The reduced gradient is considered zero and the
solution infeasible if the largest superbasic component
of the reduced gradient is less than Tol_Opt_Infeas
<p>
Pivot tolerances

<br><i>Range: [3.e-13,1]</i>

<br><i>(default = 1.e-7)</i>

<h4><a name="CONOPT4Tol_Piv_Abs">
Tol_Piv_Abs</a>
<i> (real)</i>  Absolute pivot tolerance.</h4><p>

During LU-factorization of the basis matrix a pivot
element is considered large enough if its
absolute value is larger than Tol_Piv_Abs. There is also a
relative test, see <a href="#Tol_Piv_Rel">Tol_Piv_Rel</a>.

<br><i>Range: [2.2e-16,1.e-7]</i>

<br><i>(default = 1.e-10)</i>

<h4><a name="CONOPT4Tol_Piv_Abs_Ini">
Tol_Piv_Abs_Ini</a>
<i> (real)</i>  Absolute Pivot Tolerance for building initial basis.</h4><p>

Absolute pivot tolerance used during the search for
a first logically non-singular basis. The default is fairly
large to encourage a better conditioned initial basis.

<br><i>Range: [3.e-13,1.e-3]</i>

<br><i>(default = 1.e-7)</i>

<h4><a name="CONOPT4Tol_Piv_Abs_NLTr">
Tol_Piv_Abs_NLTr</a>
<i> (real)</i>  Absolute pivot tolerance for nonlinear elements in pre-triangular equations.</h4><p>

The smallest pivot that can be used for nonlinear or
variable Jacobian elements during the pre-triangular
solve. The pivot tolerance for linear or
constant Jacobian elements is Tol_Piv_Abs. The value cannot
be less that Tol_Piv_Abs.

<br><i>Range: [2.2e-16,1.e-3]</i>

<br><i>(default = 1.e-5)</i>

<h4><a name="CONOPT4Tol_Piv_Rel">
Tol_Piv_Rel</a>
<i> (real)</i>  Relative pivot tolerance during basis factorizations.</h4><p>

During LU-factorization of the basis matrix a pivot
element is considered large enough relative to
other elements in the column if its absolute value is at
least Tol_Piv_Rel * the largest absolute value in the column.
Small values or Tol_Piv_Rel will often give a sparser basis factorization
at the expense of the numerical accuracy. The value used
internally is therefore adjusted dynamically between the
users value and 0.9, based on various statistics collected
during the solution process. Certain models derived from
finite element approximations of partial differential
equations can give rise to poor numerical accuracy and
a larger user-value of Tol_Piv_Rel may help.

<br><i>Range: [1.e-3,0.9]</i>

<br><i>(default = 0.05)</i>

<h4><a name="CONOPT4Tol_Piv_Rel_Ini">
Tol_Piv_Rel_Ini</a>
<i> (real)</i>  Relative Pivot Tolerance for building initial basis</h4><p>

Relative pivot tolerance used during the search for
a first logically non-singular basis.

<br><i>Range: [1.e-4,0.9]</i>

<br><i>(default = 1.e-3)</i>

<h4><a name="CONOPT4Tol_Piv_Rel_Updt">
Tol_Piv_Rel_Updt</a>
<i> (real)</i>  Relative pivot tolerance during basis updates.</h4><p>

During basischanges CONOPT attempts to use cheap updates of
the LU-factors of the basis. A pivot is considered large enough relative to the
alternatives in the column if its absolute value is at
least Tol_Piv_Rel_Updt * the other element. Smaller values of Tol_Piv_Rel_Updt will
allow sparser basis updates but may cause accumulation of larger
numerical errors.

<br><i>Range: [1.e-3,0.9]</i>

<br><i>(default = 0.05)</i>

<h4><a name="CONOPT4Tol_Scale2D_Min">
Tol_Scale2D_Min</a>
<i> (real)</i>  Lower bound for scale factors based on large 2nd derivatives.</h4><p>

Scaling of the model is in most cases based on the values
of the variables and the first derivatives. However,
if the scaled variables and derivatives are reasonable
but there are large values in the Hessian of the Lagrangian
(the matrix of 2nd derivatives) then the lower bound on
the scale factor can be made smaller than Tol_Scale_Min.
CONOPT will try to scale variables with large 2nd
derivatives by one over the square root of the diagonal
elements of the Hessian. However, the revised scale
factors cannot be less than Tol_Scale2D_Min.

<br><i>Range: [1.e-9,1]</i>

<br><i>(default = 1.e-6)</i>

<h4><a name="CONOPT4Tol_Scale_Max">
Tol_Scale_Max</a>
<i> (real)</i>  Upper bound on scale factors.</h4><p>

Scale factors are projected on the interval from Tol_Scale_Min to Tol_Scale_Max. Is used to
prevent very large or very small scale factors due to
pathological types of constraints. The upper limit is selected such that Square(X) can
be handled for X close to Lim_Variable. More nonlinear functions may not be scalable
for very large variables.

<br><i>Range: [1,1.e30]</i>

<br><i>(default = 1.e25)</i>

<h4><a name="CONOPT4Tol_Scale_Min">
Tol_Scale_Min</a>
<i> (real)</i>  Lower bound for scale factors computed from values and 1st derivatives.</h4><p>

Scale factors used to scale variables and equations are projected
on the range Tol_Scale_Min to Tol_Scale_Max. The limits are used to
prevent very large or very small scale factors due to
pathological types of constraints. The default value
for Tol_Scale_Min is 1 which means that small values are not
scaled up. If you need to scale small value up towards
1 then you must define a value of Tol_Scale_Min < 1.

<br><i>Range: [1.e-10,1]</i>

<br><i>(default = 1)</i>

<h4><a name="CONOPT4Tol_Scale_Var">
Tol_Scale_Var</a>
<i> (real)</i>  Lower bound on x in x*Jac used when scaling.</h4><p>

Rows are scaled so the largest term x*Jac is around 1.
To avoid difficulties with models where Jac is very large
and x very small a lower bound of Tol_Scale_Var is applied
to the x-term.
<p>
Largest Jacobian element and tolerance in 2nd derivative tests:

<br><i>Range: [1.e-8,1]</i>

<br><i>(default = 1.e-5)</i>

<h4><a name="CONOPT4Tol_Zero">
Tol_Zero</a>
<i> (real)</i>  Zero filter for Jacobian elements and inversion results.</h4><p>

Contains the smallest absolute value that an intermediate
result can have. If it is smaller, it is set to
zero. It must be smaller than <a href="#Tol_Piv_Abs">Tol_Piv_Abs</a>/10.

<br><i>(default = 1.e-20)</i>
</body></html>
